### Linear Mixed Model: Predicting P(Choose Correct) ###
Linear mixed model fit by REML. t-tests use Satterthwaite's method ['lmerModLmerTest']
Formula: mean_p_choose_corr ~ AttentionW_bin * OVcate_2 + (1 | sub_id)
   Data: agg_data

REML criterion at convergence: -380.9

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-5.0192 -0.4076  0.1427  0.6053  2.0662 

Random effects:
 Groups   Name        Variance Std.Dev.
 sub_id   (Intercept) 0.002447 0.04947 
 Residual             0.012913 0.11363 
Number of obs: 315, groups:  sub_id, 21

Fixed effects:
                                 Estimate Std. Error         df t value Pr(>|t|)    
(Intercept)                      0.858332   0.027045 221.345806  31.737  < 2e-16 ***
AttentionW_bin2                  0.023796   0.035068 280.000000   0.679  0.49797    
AttentionW_bin3                 -0.177859   0.035068 280.000000  -5.072 7.18e-07 ***
AttentionW_bin4                 -0.057179   0.035068 280.000000  -1.631  0.10412    
AttentionW_bin5                  0.007007   0.035068 280.000000   0.200  0.84178    
OVcate_2medium                   0.057490   0.035068 280.000000   1.639  0.10226    
OVcate_2high                    -0.112660   0.035068 280.000000  -3.213  0.00147 ** 
AttentionW_bin2:OVcate_2medium  -0.008276   0.049594 280.000000  -0.167  0.86759    
AttentionW_bin3:OVcate_2medium   0.183491   0.049594 280.000000   3.700  0.00026 ***
AttentionW_bin4:OVcate_2medium   0.106218   0.049594 280.000000   2.142  0.03308 *  
AttentionW_bin5:OVcate_2medium   0.049834   0.049594 280.000000   1.005  0.31584    
AttentionW_bin2:OVcate_2high     0.101060   0.049594 280.000000   2.038  0.04252 *  
AttentionW_bin3:OVcate_2high     0.331214   0.049594 280.000000   6.678 1.30e-10 ***
AttentionW_bin4:OVcate_2high     0.124779   0.049594 280.000000   2.516  0.01243 *  
AttentionW_bin5:OVcate_2high     0.080801   0.049594 280.000000   1.629  0.10439    
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

### Pairwise Comparisons (Bonferroni-adjusted) - Attentional Weight Bins Within OV Levels ###
$emmeans
OVcate_2 = low:
 AttentionW_bin emmean    SE  df lower.CL upper.CL
 1               0.858 0.027 221    0.805    0.912
 2               0.882 0.027 221    0.829    0.935
 3               0.680 0.027 221    0.627    0.734
 4               0.801 0.027 221    0.748    0.854
 5               0.865 0.027 221    0.812    0.919

OVcate_2 = medium:
 AttentionW_bin emmean    SE  df lower.CL upper.CL
 1               0.916 0.027 221    0.863    0.969
 2               0.931 0.027 221    0.878    0.985
 3               0.921 0.027 221    0.868    0.975
 4               0.965 0.027 221    0.912    1.018
 5               0.973 0.027 221    0.919    1.026

OVcate_2 = high:
 AttentionW_bin emmean    SE  df lower.CL upper.CL
 1               0.746 0.027 221    0.692    0.799
 2               0.871 0.027 221    0.817    0.924
 3               0.899 0.027 221    0.846    0.952
 4               0.813 0.027 221    0.760    0.867
 5               0.833 0.027 221    0.780    0.887

Degrees-of-freedom method: kenward-roger 
Confidence level used: 0.95 

$contrasts
OVcate_2 = low:
 contrast                          estimate     SE  df t.ratio p.value
 AttentionW_bin1 - AttentionW_bin2 -0.02380 0.0351 280  -0.679  1.0000
 AttentionW_bin1 - AttentionW_bin3  0.17786 0.0351 280   5.072  <.0001
 AttentionW_bin1 - AttentionW_bin4  0.05718 0.0351 280   1.631  1.0000
 AttentionW_bin1 - AttentionW_bin5 -0.00701 0.0351 280  -0.200  1.0000
 AttentionW_bin2 - AttentionW_bin3  0.20166 0.0351 280   5.750  <.0001
 AttentionW_bin2 - AttentionW_bin4  0.08098 0.0351 280   2.309  0.2167
 AttentionW_bin2 - AttentionW_bin5  0.01679 0.0351 280   0.479  1.0000
 AttentionW_bin3 - AttentionW_bin4 -0.12068 0.0351 280  -3.441  0.0067
 AttentionW_bin3 - AttentionW_bin5 -0.18487 0.0351 280  -5.272  <.0001
 AttentionW_bin4 - AttentionW_bin5 -0.06419 0.0351 280  -1.830  0.6827

OVcate_2 = medium:
 contrast                          estimate     SE  df t.ratio p.value
 AttentionW_bin1 - AttentionW_bin2 -0.01552 0.0351 280  -0.443  1.0000
 AttentionW_bin1 - AttentionW_bin3 -0.00563 0.0351 280  -0.161  1.0000
 AttentionW_bin1 - AttentionW_bin4 -0.04904 0.0351 280  -1.398  1.0000
 AttentionW_bin1 - AttentionW_bin5 -0.05684 0.0351 280  -1.621  1.0000
 AttentionW_bin2 - AttentionW_bin3  0.00989 0.0351 280   0.282  1.0000
 AttentionW_bin2 - AttentionW_bin4 -0.03352 0.0351 280  -0.956  1.0000
 AttentionW_bin2 - AttentionW_bin5 -0.04132 0.0351 280  -1.178  1.0000
 AttentionW_bin3 - AttentionW_bin4 -0.04341 0.0351 280  -1.238  1.0000
 AttentionW_bin3 - AttentionW_bin5 -0.05121 0.0351 280  -1.460  1.0000
 AttentionW_bin4 - AttentionW_bin5 -0.00780 0.0351 280  -0.222  1.0000

OVcate_2 = high:
 contrast                          estimate     SE  df t.ratio p.value
 AttentionW_bin1 - AttentionW_bin2 -0.12486 0.0351 280  -3.560  0.0043
 AttentionW_bin1 - AttentionW_bin3 -0.15335 0.0351 280  -4.373  0.0002
 AttentionW_bin1 - AttentionW_bin4 -0.06760 0.0351 280  -1.928  0.5491
 AttentionW_bin1 - AttentionW_bin5 -0.08781 0.0351 280  -2.504  0.1285
 AttentionW_bin2 - AttentionW_bin3 -0.02850 0.0351 280  -0.813  1.0000
 AttentionW_bin2 - AttentionW_bin4  0.05726 0.0351 280   1.633  1.0000
 AttentionW_bin2 - AttentionW_bin5  0.03705 0.0351 280   1.056  1.0000
 AttentionW_bin3 - AttentionW_bin4  0.08575 0.0351 280   2.445  0.1509
 AttentionW_bin3 - AttentionW_bin5  0.06555 0.0351 280   1.869  0.6265
 AttentionW_bin4 - AttentionW_bin5 -0.02021 0.0351 280  -0.576  1.0000

Degrees-of-freedom method: kenward-roger 
P value adjustment: bonferroni method for 10 tests 


### Pairwise Comparisons (Bonferroni-adjusted) - OV Levels Within Each Bin ###
$emmeans
AttentionW_bin = 1:
 OVcate_2 emmean    SE  df lower.CL upper.CL
 low       0.858 0.027 221    0.805    0.912
 medium    0.916 0.027 221    0.863    0.969
 high      0.746 0.027 221    0.692    0.799

AttentionW_bin = 2:
 OVcate_2 emmean    SE  df lower.CL upper.CL
 low       0.882 0.027 221    0.829    0.935
 medium    0.931 0.027 221    0.878    0.985
 high      0.871 0.027 221    0.817    0.924

AttentionW_bin = 3:
 OVcate_2 emmean    SE  df lower.CL upper.CL
 low       0.680 0.027 221    0.627    0.734
 medium    0.921 0.027 221    0.868    0.975
 high      0.899 0.027 221    0.846    0.952

AttentionW_bin = 4:
 OVcate_2 emmean    SE  df lower.CL upper.CL
 low       0.801 0.027 221    0.748    0.854
 medium    0.965 0.027 221    0.912    1.018
 high      0.813 0.027 221    0.760    0.867

AttentionW_bin = 5:
 OVcate_2 emmean    SE  df lower.CL upper.CL
 low       0.865 0.027 221    0.812    0.919
 medium    0.973 0.027 221    0.919    1.026
 high      0.833 0.027 221    0.780    0.887

Degrees-of-freedom method: kenward-roger 
Confidence level used: 0.95 

$contrasts
AttentionW_bin = 1:
 contrast      estimate     SE  df t.ratio p.value
 low - medium   -0.0575 0.0351 280  -1.639  0.3068
 low - high      0.1127 0.0351 280   3.213  0.0044
 medium - high   0.1702 0.0351 280   4.852  <.0001

AttentionW_bin = 2:
 contrast      estimate     SE  df t.ratio p.value
 low - medium   -0.0492 0.0351 280  -1.403  0.4848
 low - high      0.0116 0.0351 280   0.331  1.0000
 medium - high   0.0608 0.0351 280   1.734  0.2520

AttentionW_bin = 3:
 contrast      estimate     SE  df t.ratio p.value
 low - medium   -0.2410 0.0351 280  -6.872  <.0001
 low - high     -0.2186 0.0351 280  -6.232  <.0001
 medium - high   0.0224 0.0351 280   0.640  1.0000

AttentionW_bin = 4:
 contrast      estimate     SE  df t.ratio p.value
 low - medium   -0.1637 0.0351 280  -4.668  <.0001
 low - high     -0.0121 0.0351 280  -0.346  1.0000
 medium - high   0.1516 0.0351 280   4.323  0.0001

AttentionW_bin = 5:
 contrast      estimate     SE  df t.ratio p.value
 low - medium   -0.1073 0.0351 280  -3.060  0.0073
 low - high      0.0319 0.0351 280   0.908  1.0000
 medium - high   0.1392 0.0351 280   3.969  0.0003

Degrees-of-freedom method: kenward-roger 
P value adjustment: bonferroni method for 3 tests 


### Model Comparison: Does OV Level Influence Attentional Weight Effects? ###
Data: agg_data
Models:
lmm_no_OV: mean_p_choose_corr ~ AttentionW_bin + (1 | sub_id)
lmm_model: mean_p_choose_corr ~ AttentionW_bin * OVcate_2 + (1 | sub_id)
          npar     AIC     BIC logLik deviance  Chisq Df Pr(>Chisq)    
lmm_no_OV    7 -337.32 -311.05 175.66  -351.32                         
lmm_model   17 -429.27 -365.48 231.64  -463.27 111.95 10  < 2.2e-16 ***
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1

### Linear Mixed Model (Continuous Attentional Weight) ###
Linear mixed model fit by REML. t-tests use Satterthwaite's method ['lmerModLmerTest']
Formula: corr ~ AttentionW_bin * OVcate_2 + (1 | sub_id)
   Data: data

REML criterion at convergence: 4259.4

Scaled residuals: 
    Min      1Q  Median      3Q     Max 
-3.2359  0.0706  0.2792  0.4969  1.3523 

Random effects:
 Groups   Name        Variance Std.Dev.
 sub_id   (Intercept) 0.002822 0.05312 
 Residual             0.103611 0.32189 
Number of obs: 7234, groups:  sub_id, 21

Fixed effects:
                                 Estimate Std. Error         df t value Pr(>|t|)    
(Intercept)                       0.86198    0.01739   82.57107  49.583  < 2e-16 ***
AttentionW_bin2                   0.04809    0.02017 7213.29685   2.384  0.01717 *  
AttentionW_bin3                  -0.16913    0.01988 7202.45377  -8.508  < 2e-16 ***
AttentionW_bin4                  -0.06301    0.01935 7201.88296  -3.257  0.00113 ** 
AttentionW_bin5                   0.00311    0.01932 7201.89413   0.161  0.87212    
OVcate_2medium                    0.05312    0.01848 7202.56788   2.875  0.00406 ** 
OVcate_2high                     -0.11912    0.02085 7201.25994  -5.713 1.16e-08 ***
AttentionW_bin2:OVcate_2medium   -0.03409    0.02781 7208.96999  -1.226  0.22028    
AttentionW_bin3:OVcate_2medium    0.17221    0.02751 7200.61841   6.260 4.06e-10 ***
AttentionW_bin4:OVcate_2medium    0.11239    0.02701 7200.79141   4.161 3.21e-05 ***
AttentionW_bin5:OVcate_2medium    0.05318    0.02709 7201.01701   1.963  0.04966 *  
AttentionW_bin2:OVcate_2high      0.08406    0.02988 7206.85488   2.813  0.00492 ** 
AttentionW_bin3:OVcate_2high      0.32168    0.03204 7203.33639  10.040  < 2e-16 ***
AttentionW_bin4:OVcate_2high      0.13226    0.03031 7200.71765   4.364 1.30e-05 ***
AttentionW_bin5:OVcate_2high      0.08129    0.03027 7200.24122   2.686  0.00725 ** 
---
Signif. codes:  0 ‘***’ 0.001 ‘**’ 0.01 ‘*’ 0.05 ‘.’ 0.1 ‘ ’ 1
